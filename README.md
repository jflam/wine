# wine

## Experiments in building personal search engines

The original inspiration for this work is this Toward Data Science
[article](https://towardsdatascience.com/the-auto-sommelier-how-to-implement-huggingface-transformers-and-build-a-search-engine-9e0f401b1bda).
This article opened my eyes to what might be possible in writing search
engines using word embeddings generated by modern models like
[transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).

I've been interested in search engines for quite a while now and have built
some rudimentary search engines from scratch in Python. This repo contains
experiments in building search engines using more modern techniques.

## Sentence-Transformers

The main documentation for [sentence transformers](https://www.sbert.net/).
One of the key things that it powers is [semantic search](https://www.sbert.net/examples/applications/semantic-search/README.html)

The models described in the page for semantic search differ based on their
applications. Symmetric cases are where the length of the query text and the
retrieval text are similar, and [these models perform
well](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models).


In asymmetric cases where the length of the query is different from the length
of the text retrieved, [these models perform
better](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models).
It is worth noting that many of the models came from work done by the Bing
search engine team.

In all cases, these models use GPU acceleration. Performance is quite good on
my desktop GPU, an RTX 2080, with typical query results returned in <20ms. I
have not tried to test these models on machines that lack GPU support.

The model that is currently used is `msmarco-distilbert-base-v4`. These models
use the [ms-marco datasets](https://microsoft.github.io/msmarco/) which 
started as a 100,000 question/answer dataset from Bing, which has since been
expanded to a 1,000,000 question/answer dataset. These datasets have been
used extensively in academic research and the linked page above contains 
leaderboards for progress in different text retrieval tasks.